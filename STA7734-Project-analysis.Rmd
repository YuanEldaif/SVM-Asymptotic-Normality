---
title: "STA7734 Project"
author: "Yuan Du"
date: "11/26/2019"
output:html_document: default
---

### Appendix
```{r setup, eval=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(psych)
library(corrplot)
library(e1071)
library(caret)
library(plotly)
library(plyr)

```

## Read data
```{r read, message=FALSE, warning=FALSE, eval=FALSE}
Data <- read_csv("C:/Work/Project/STA7734/SVM-Asymptotic-Normality/Data/data.csv",
                 col_names = T,
                 col_types=cols(ID =col_character()，
                                Diagnosis=col_character(),
                                "3" = col_double(),
                                "4" = col_double(),
                                "5" = col_double(),
                                "6" = col_double(),
                                "7" = col_double(),
                                "8" = col_double(),
                                "9" = col_double(),
                                "10" = col_double(),
                                "11" = col_double(),
                                "12" = col_double(),
                                "13" = col_double(),
                                "14" = col_double(),
                                "15" = col_double(),
                                "16" = col_double(),
                                "17" = col_double(),
                                "18" = col_double(),
                                "19" = col_double(),
                                "20" = col_double(),
                                "21" = col_double(),
                                "22" = col_double(),
                                "23" = col_double(),
                                "24" = col_double(),
                                "25" = col_double(),
                                "26" = col_double(),
                                "27" = col_double(),
                                "28" = col_double(),
                                "29" = col_double(),
                                "30" = col_double(),
                                "31" = col_double(),
                                "32" = col_double()
                                )) %>%
                        dplyr::rename( "radius" = "3",
                                "texture" = "4",
                                "perimeter" = "5",
                                "area" = "6",
                                "smoothness" = "7",
                                "compactness" = "8",
                                "concavity" = "9",
                                "concave_points" = "10",
                                "symmetry" = "11",
                                "fractal_dimension" = "12",
                                "radiusSE" = "13",
                                "textureSE" = "14",
                                "perimeterSE" = "15",
                                "areaSE" = "16",
                                "smoothnessSE" = "17",
                                "compactnessSE" = "18",
                                "concavitySE" = "19",
                                "concave_pointsSE" = "20",
                                "symmetrySE" = "21",
                                "fractal_dimensionSE" = "22",
                                "radiusW" = "23",
                                "textureW" = "24",
                                "perimeterW" = "25",
                                "areaW" = "26",
                                "smoothnessW" = "27",
                                "compactnessW" = "28",
                                "concavityW" = "29",
                                "concave_pointsW" = "30",
                                "symmetryW" = "31",
                                "fractal_dimensionW" = "32")

#Check specs/data type of the data:
spec(Data)
```

## Check first 6 rows of Data, and missing values 
## * 32 columns, first two columns are ID and target variable: Diagnosis (B,M).
## * No Missing value
```{r check, eval=FALSE}
head(Data)
#tail(Data)
sapply(Data, function(x) sum(is.na(x)))
```

## Descriptive Statistics on numeric variables and target variable: Diagnosis
```{r, message=FALSE, warning=FALSE, eval=FALSE}

Data_drop <- Data[,-1] #drop ID
psych::describe(Data_drop) # Describe data
#psych::describeBy(Data_drop,Data_drop$Diagnosis) #Describe data by Diagnosis

Data_drop$Diagnosis <- as.factor(Data_drop$Diagnosis)
summary(Data_drop$Diagnosis)
```
# Data visiualization
## * Plot Diagnosis; 
## * Histogram for all Variables; 
## * Histogram for all Variables by Diagnosis.

## **There are differences between B and M Diagnosis such as area, compactness, perimeter, radius, etc**
```{r, message=FALSE, warning=FALSE, eval=FALSE}
ggplot(Data,aes(x = Diagnosis）) +
    geom_bar(aes(y = ..count../sum(..count..), fill = Diagnosis)) + 
        ylab("Percent") + 
            ggtitle("Diagnosis percentages")
    

#Plot histogram for all Variables
Data_drop %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    geom_histogram() + 
    facet_wrap(~key, scales = "free")

#Plot histogram for all Variables by Diagnosis
Data_drop %>%
  gather(key = "Var", value = "value", -Diagnosis) %>% 
  ggplot(aes(x=value, fill=Diagnosis)) + 
    geom_histogram(bins = 60) + 
    facet_wrap(~Var, scales = "free")
```

## Correlaion analysis by spearman since most variables are non normal distributed
```{r, eval=FALSE}
#create correlation matrix
Cor <- Data_drop %>% 
       select(-Diagnosis) %>%
       cor(method="spearman")

corrplot(Cor, type="upper", order="hclust")
```

## Drop variables with multicolinearity, where correlation >0.9, and remove the variable with the largest mean absolute correlation
```{r, eval=FALSE}
#Removing all features with a correlation higher than 0.7, keeping the feature with the lower mean. 

highlyCor <- colnames(Data_drop[,-1])[findCorrelation(Cor, cutoff = 0.9, verbose = TRUE)]
```

## Highly correlated variables
```{r, eval=FALSE}
highlyCor #highly correlted variables

Data_drop_cor <- Data_drop[, which(!colnames(Data_drop) %in% highlyCor)]#Drop highly correlcted variables

```

## Number of features left
```{r, eval=FALSE}
ncol(Data_drop_cor)-1
```

## Data Transformation/ Dimension Reduction by using PCA, after dropping label.
```{r, eval=FALSE}
#Drop Diagnosis
Data_pca <- prcomp(Data_drop[,-1], center=TRUE, scale=TRUE)
```

## PCA components summary for raw data
```{r, eval=FALSE}
summary(Data_pca)#need 10 components for variance over 95%
```

## PCA without highly correlated variables
```{r, eval=FALSE}
Data_pca_re <- prcomp(Data_drop_cor[,-1], center=TRUE, scale=TRUE)
summary(Data_pca_re)#7 components have variance over 97%
```

## Plot first 2 PCA components
```{r, eval=FALSE}
pca_df <- as.data.frame(Data_pca$x)
ggplot(pca_df, aes(x=PC1, y=PC2, col=Data$Diagnosis)) + geom_point(alpha=0.5)
```

## Plot PCA without highly correlated variables on the first 2 components
```{r, eval=FALSE}
pca_df_re <- as.data.frame(Data_pca_re$x)
ggplot(pca_df_re, aes(x=PC1, y=PC2, col=Data$Diagnosis)) + geom_point(alpha=0.5)
```

## Visualize which variables are the most influential. Individuals with a similar profile are grouped together
```{r, message = FALSE, eval=FALSE }
library(factoextra)
fviz_eig(Data_pca)
fviz_pca_var(Data_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

## After correlated variables were dropped
```{r, eval=FALSE}
fviz_eig(Data_pca_re)
fviz_pca_var(Data_pca_re,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```

## **Correlated variables such as area, perimeters, radius are grouped together and they are important contributers**
<br />
# **Model Training**
## split data into training and test
```{r, eval=FALSE}

set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(Data_drop$Diagnosis, p=.8, list=F)

train_raw = Data_drop %>% 
  slice(trainIndices)

nrow(train_raw)

test_raw = Data_drop %>% 
  slice(-trainIndices)
head(test_raw)

#verify if the randomization process is correct
prop.table(table(train_raw$Diagnosis))
prop.table(table(test_raw$Diagnosis))
```

## split data into training and test after dropping correlated variables
```{r, eval=FALSE}
#for multicolinearty droped Data_drop_cor
set.seed(1234) # so that the indices will be the same when re-run
trainIndices_drop = createDataPartition(Data_drop_cor$Diagnosis, p=.8, list=F)

train_drop_raw = Data_drop_cor %>% 
  slice(trainIndices_drop)

nrow(train_drop_raw)
head(train_drop_raw)

test_drop_raw = Data_drop_cor %>% 
  slice(-trainIndices_drop)
head(test_drop_raw)

#verify if the randomization process is correct
prop.table(table(train_drop_raw$Diagnosis))
prop.table(table(test_drop_raw$Diagnosis))
```

## Svm with original data by Using library(e1071), data is not scaled; 
## * Default parameters: Cost =1, gamma = 1/(data dimension)
## first gamma:
```{r svm raw, eval=FALSE}

svm_raw <- svm(Diagnosis ~ ., data=train_raw, scale = FALSE, kernel =
"radial", type = "C-classification")
#svm_pca <- svm(Diagnosis ~ ., data=Data_pca)
svm_raw$gamma

pred_svm <- predict(svm_raw, test_raw)

cm_svm <- confusionMatrix(pred_svm, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm
#summary(svm_raw)
```

## Svm with after droped correlated variables with default setting by using library(e1071)
## second gamma:
```{r svm_drop_raw, eval=FALSE}
svm_drop_raw <- svm(Diagnosis ~ ., data=train_drop_raw, scale = FALSE, kernel =
"radial", type = "C-classification")
#svm_pca <- svm(Diagnosis ~ ., data=Data_pca)
svm_drop_raw$gamma

pred_drop_svm <- predict(svm_drop_raw, test_drop_raw)

cm_drop_svm <- confusionMatrix(pred_drop_svm, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_drop_svm
```

## Use caret package to fit models with default parameter tuning  with 5 fold cross validation, repeated 5 times; Caret "svmRadial" model uses kernlab package; No data scaling.

### * $C = 2^{((1:len) - 3)}$ (C=0.25) 
### * Sigma is defined by Kernlab package's sigest function, sigest estimates are based upon the 0.1 and 0.9 quantile of $||x-x'||^2$. Basically any value in between those two bounds will produce good results. A vector of length 3 defining the range (0.1 quantile, median and 0.9 quantile) of the sigma hyperparameter.
```{r svm caret raw, eval=FALSE}
#fitControl <- trainControl(method = "none", classProbs = TRUE)
fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_raw_caret <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    trace=FALSE,
                    trControl=fitControl)
svm_raw_caret$finalModel
pred_svm_caret <- predict(svm_raw_caret, test_raw)
cm_svm_caret <- confusionMatrix(pred_svm_caret, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_caret

```
## Use caret package to fit models after droped correlated variables with default parameter tuning with 5 fold cross validation, repeated 5 times; No data scaling.

### **It's the same result with previous model without dropped correlated variables but it's less sensitive on positive predicting and more sensitive on negative predicting.**
```{r svm caret drop raw, eval=FALSE}
#fitControl <- trainControl(method = "none", classProbs = TRUE)
fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_drop_caret <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="svmRadial",
                    metric="ROC",
                    trace=FALSE,
                    trControl=fitControl)
svm_drop_caret$finalModel
pred_drop_svm_caret <- predict(svm_drop_caret, test_drop_raw)
cm_drop_svm_caret <- confusionMatrix(pred_drop_svm_caret, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_drop_svm_caret
```

# **Scale variables and cross validation**
```{r svm scale, eval=FALSE}

fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_scale <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    trControl=fitControl)
svm_scale$finalModel
pred_svm_scale <- predict(svm_scale, test_raw)
cm_svm_scale <- confusionMatrix(pred_svm_scale, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_scale

```

## Scale variables with pca thresdhold = 0.8. 0.8 is the sweet spot.
```{r svm scale pca, eval=FALSE}

fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_scale_pca <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale','pca'),
                    trace=FALSE,
                    trControl=fitControl)
svm_scale_pca$finalModel
pred_svm_scale_pca <- predict(svm_scale_pca, test_raw)
cm_svm_scale_pca <- confusionMatrix(pred_svm_scale_pca, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_scale_pca

```

## Scale variables with dropped variables and with pca = 0.95; 

### **The results doesn't improve much anymore, with increasing threshold of pca, the performance increases on the 4th digit and it's more stable**
```{r svm pca, eval=FALSE}
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.95), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_drop_pca  <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale','pca'),
                    trace=FALSE,
                    trControl=fitControl
                    #tuneLength = 8
                    )
svm_drop_pca
pred_svmdrop_pca <- predict(svm_drop_pca, train_drop_raw)
cm_svmdrop_pca <- confusionMatrix(pred_svmdrop_pca, as.factor(train_drop_raw$Diagnosis), positive = "M")
cm_svmdrop_pca

```

# Tune SVM parameters by grid search, total 42 pairs

```{r, echo=TRUE }
svmGrid <-  expand.grid(sigma = c(.01, .015, 0.2,0.25, 0.275,0.3),
                    C = c(0.25, 0.5, 0.75, 0.9, 1, 1.1, 1.25))
                    
nrow(svmGrid)
```


```{r, eval=FALSE }
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_tune <- train(Diagnosis ~ ., 
                 data = train_drop_raw, 
                 method="svmRadial",
                 metric="ROC",
                 preProcess=c('center', 'scale','pca'),
                 trace=FALSE,
                 trControl=fitControl，
                 tuneGrid = svmGrid)
#svm_tune
#ten fold The final values used for the model were sigma = 0.015 and C = 0.9.
#five fold The final values used for the model were sigma = 0.015 and C = 0.75.
#The final values used for the model were sigma = 0.015 and C = 1.

```

## Plot the results on the feature selected data with pca, by grid search 
```{r, eval=FALSE}
plot(svm_tune)  
```

## Performance on test data with the best parameters
```{r, eval=FALSE}
pred_svm_tune <- predict(svm_tune, train_drop_raw)
cm_svm_tune <- confusionMatrix(pred_svm_tune, as.factor(train_drop_raw$Diagnosis), positive = "M")
cm_svm_tune

#Plotting the Resampling Profile: Examine the relationship between the estimates of performance and the tuning parameters

```

### **The best parameters on training data doesn't guarentee the best performance on the test data. It could be easily overfitting. The final values used for the model were sigma = 0.015 and C = 1.**
### **Instead of just doing grid search, could we add another regulization papameter to reduce over fitting, or **
### **instead of choosing the best pair of parameters (C, sigma), allow more errors and use another algorithm, for example, select top 3 pairs on test dataset to find the best pair of parameters?**

## Neural network
```{r neural net scale, eval=FALSE}
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
model_nnet <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="nnet",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    tuneLength=10,
                    trControl=fitControl)
#model_nnet
pred_nnet <- predict(model_nnet, test_drop_raw)
cm_nnet <- confusionMatrix(pred_nnet, as.factor(test_drop_raw$Diagnosis), positive = "M")

# #grid search for nnet
# nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
#                         decay = seq(from = 0.1, to = 0.5, by = 0.1))
```
```{r, eval=FALSE}
cm_nnet
```

## Neural network with pca
```{r neural net scale pca, eval=FALSE}
set.seed(825)
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
nnet_pca <- train(Diagnosis~.,
                 data = train_raw,
                 method = "nnet",
                 metric = "ROC",
                 preProcess=c('center', 'scale','pca'),
                 trace=FALSE,
                 tuneLength=10,
                 trControl = fitControl)
#nnet_pca
pred_nnet_pca <- predict(nnet_pca, test_raw)
cm_nnet_pca <- confusionMatrix(pred_nnet_pca, as.factor(test_raw$Diagnosis), positive = "M")
cm_nnet_pca 

```
```{r, eval=FALSE}
cm_nnet_pca 
```


# Models evaluation by caret package with cross validation on training model
```{r, eval=FALSE}
model_list <- list(Caret_svm_scale= svm_scale, Caret_svm_scale_pca=svm_scale_pca, 
                   Caret_drop_svm_pca=svm_drop_pca, Caret_nnet_pca=nnet_pca)
resamples <- resamples(model_list)
bwplot(resamples, metric = "ROC")
```

# Overall model evaluation on test data
## **Final model achieved accuracy of 0.9825.**
```{r, eval=FALSE}
overall <- data.frame(model = rep(c("Svm_raw", "Svm_drop","Caret_svm_scale", "Caret_svm_scale_pca", "Caret_drop_svm_pca", "Caret_nnet_pca")),
                      rbind(cm_svm$overall,
                            cm_drop_svm$overall,
                            cm_svm_scale$overall,
                            cm_svm_scale_pca$overall,
                            cm_svmdrop_pca$overall,
                            cm_nnet_pca$overall))
#overall
overall_gather <- overall[,1:3] %>%
  gather(measure, value, Accuracy:Kappa)
#overall_gather
byClass <- data.frame(model = rep(c("Svm_raw", "Svm_drop","Caret_svm_scale", "Caret_svm_scale_pca", "Caret_drop_svm_pca", "Caret_nnet_pca")),
                      rbind(cm_svm$byClass,
                            cm_drop_svm$byClass,
                            cm_svm_scale$byClass,
                            cm_svm_scale_pca$byClass,
                            cm_svmdrop_pca$byClass,
                            cm_nnet_pca$byClass))
#byClass  
byClass_gather <- byClass[,1:3] %>%
  gather(measure, value, Sensitivity:Specificity)
#byClass_gather

overall_byClass_gather <- rbind(overall_gather, byClass_gather)
overall_byClass_gather <- within(overall_byClass_gather, model <- factor(model, levels = c("Svm_raw", "Svm_drop","Caret_svm_scale", "Caret_nnet_pca", "Caret_svm_scale_pca", "Caret_drop_svm_pca")))

#overall_byClass_gather
OV<-overall_byClass_gather %>% 
  filter(measure == "Accuracy")  %>% 
  arrange(value)

#OV

Per<-ggplot(OV, aes(x = model, y = value)) +
    geom_point(shape=1)  +
    scale_x_discrete(labels = abbreviate) +
    scale_y_continuous(breaks=c(0.6,0.8, 0.94, 0.96,0.97,0.98, 0.99)) 

Performance <- ggplotly(Per) %>%
  layout(title = 'Interactive Model Performance',
         xaxis = list(title = 'Model'),
         yaxis = list (title = 'Accuracy'))
Performance
```

# **Takeaways:**
### 1. Scaling variables is very important. and tuning parameters by using resampling technique for example, cross validation is more important.

### 2. Reduce multicolinearity could stablize the model performance.

### 3. Dimention deduction (using PCA here) with out feature selection is not robust. It would takes time to find a sweet spot on the optimal PCA value.

### 4. In another word, with the appropriate feature selection, it reduces the variation of dimention deduction with a stable result. 

### 5. The best tuning parameters on training could be overfitting. Neural network tends to overfit the model.

### 6. Grid search method could be improved by combing performance on test data instead of sololy relys on training data.

### 7. A good SVM could perform better than neurual network and could explain important features.




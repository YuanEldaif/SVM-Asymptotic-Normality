---
title: "STA7734 Project"
author: "Yuan Du"
date: "11/26/2019"
output:
  
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(psych)
library(corrplot)
library(caret)
```

## read data
```{r read, message=FALSE, warning=FALSE, include=FALSE}
Data <- read_csv("C:/Work/Project/STA7734/SVM-Asymptotic-Normality/Data/data.csv",
                 col_names = T,
                 col_types=cols(ID =col_character()，
                                Diagnosis=col_character(),
                                "3" = col_double(),
                                "4" = col_double(),
                                "5" = col_double(),
                                "6" = col_double(),
                                "7" = col_double(),
                                "8" = col_double(),
                                "9" = col_double(),
                                "10" = col_double(),
                                "11" = col_double(),
                                "12" = col_double(),
                                "13" = col_double(),
                                "14" = col_double(),
                                "15" = col_double(),
                                "16" = col_double(),
                                "17" = col_double(),
                                "18" = col_double(),
                                "19" = col_double(),
                                "20" = col_double(),
                                "21" = col_double(),
                                "22" = col_double(),
                                "23" = col_double(),
                                "24" = col_double(),
                                "25" = col_double(),
                                "26" = col_double(),
                                "27" = col_double(),
                                "28" = col_double(),
                                "29" = col_double(),
                                "30" = col_double(),
                                "31" = col_double(),
                                "32" = col_double()
                                )) %>%
                        rename( "radius" = "3",
                                "texture" = "4",
                                "perimeter" = "5",
                                "area" = "6",
                                "smoothness" = "7",
                                "compactness" = "8",
                                "concavity" = "9",
                                "concave_points" = "10",
                                "symmetry" = "11",
                                "fractal_dimension" = "12",
                                "radiusSE" = "13",
                                "textureSE" = "14",
                                "perimeterSE" = "15",
                                "areaSE" = "16",
                                "smoothnessSE" = "17",
                                "compactnessSE" = "18",
                                "concavitySE" = "19",
                                "concave_pointsSE" = "20",
                                "symmetrySE" = "21",
                                "fractal_dimensionSE" = "22",
                                "radiusW" = "23",
                                "textureW" = "24",
                                "perimeterW" = "25",
                                "areaW" = "26",
                                "smoothnessW" = "27",
                                "compactnessW" = "28",
                                "concavityW" = "29",
                                "concave_pointsW" = "30",
                                "symmetryW" = "31",
                                "fractal_dimensionW" = "32")

#Check specs/data type of the data:
spec(Data)
```

## Check Data and missing values
```{r check, echo=FALSE}
head(Data)
tail(Data)
sapply(Data, function(x) sum(is.na(x)))
```

## Data visiualization
```{r, message=FALSE, warning=FALSE, echo=TRUE}

Data_drop <- Data[,-1] #drop ID
psych::describe(Data_drop) # Describe data
#psych::describeBy(Data_drop,Data_drop$Diagnosis) #Describe data by Diagnosis

Data_drop$Diagnosis <- as.factor(Data_drop$Diagnosis)
summary(Data_drop$Diagnosis)

#Plot Diagnosis 

ggplot(Data,aes(x = Diagnosis）) +
    geom_bar(aes(y = ..count../sum(..count..), fill = Diagnosis)) + 
        ylab("Percent") + 
            ggtitle("Diagnosis percentages")
    

#Plot histogram for all Variables
Data_drop %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    geom_histogram() + 
    facet_wrap(~key, scales = "free")

#Plot histogram for all Variables by Diagnosis
Data_drop %>%
  gather(key = "Var", value = "value", -Diagnosis) %>% 
  ggplot(aes(x=value, fill=Diagnosis)) + 
    geom_histogram(bins = 60) + 
    facet_wrap(~Var, scales = "free")
```

## Correlaion analysis by spearman since most variables are non normal distributed
```{r, echo=TRUE}
#create correlation matrix
Cor <- Data_drop %>% 
       select(-Diagnosis) %>%
       cor(method="spearman")

corrplot(Cor, type="upper", order="hclust")
```

## Drop variables with multicolinearity, where correlation >0.9, and remove the variable with the largest mean absolute correlation
```{r, echo=TRUE}
#Removing all features with a correlation higher than 0.7, keeping the feature with the lower mean. 

highlyCor <- colnames(Data_drop[,-1])[findCorrelation(Cor, cutoff = 0.9, verbose = TRUE)]
highlyCor #highly correlted variables

Data_drop_cor <- Data_drop[, which(!colnames(Data_drop) %in% highlyCor)]#Drop highly correlcted variables

```

##Number of columns
```{r, echo=TRUE}
ncol(Data_drop_cor)
```

## Data Transformation/ Dimension reduction by using PCA, after dropping label.
```{r, include =FALSE}
#Drop Diagnosis
Data_pca <- prcomp(Data_drop[,-1], center=TRUE, scale=TRUE)
```
#Plot first 10 PCA components for raw data
```{r, echo=TRUE}
plot(Data_pca, type="l", main='')
grid(nx = 10, ny = 14)
title(main = "Principal components weight", sub = NULL, xlab = "Components")
box()
summary(Data_pca)#need 10 components for variance over 95%
```
##PCA without multicolinearity variables
```{r, echo=TRUE}
Data_pca_re <- prcomp(Data_drop_cor[,-1], center=TRUE, scale=TRUE)
summary(Data_pca_re)#7 components have variance over 97%
```

## visualize which variables are the most influential on the first 2 components
```{r, echo=TRUE}
pca_df <- as.data.frame(Data_pca$x)
ggplot(pca_df, aes(x=PC1, y=PC2, col=Data$Diagnosis)) + geom_point(alpha=0.5)
```
##Plot PCA without multicolinearity variables on the first 2 components
```{r, echo=TRUE}
pca_df_re <- as.data.frame(Data_pca_re$x)
ggplot(pca_df_re, aes(x=PC1, y=PC2, col=Data$Diagnosis)) + geom_point(alpha=0.5)
```

## Individuals with a similar profile are grouped together
```{r, echo=TRUE }
library(factoextra)
fviz_eig(Data_pca)
fviz_pca_var(Data_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```
##PCA after droped correlated variables
```{r, echo=TRUE}
fviz_eig(Data_pca_re)
fviz_pca_var(Data_pca_re,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

```

#split data into training and test
```{r, include=FALSE}

library(caret)
set.seed(1234) # so that the indices will be the same when re-run
trainIndices = createDataPartition(Data_drop$Diagnosis, p=.8, list=F)

train_raw = Data_drop %>% 
  slice(trainIndices)

nrow(train_raw)

test_raw = Data_drop %>% 
  slice(-trainIndices)
head(test_raw)

#verify if the randomization process is correct
prop.table(table(train_raw$Diagnosis))
prop.table(table(test_raw$Diagnosis))
```

#split data into training and test after dropping correlated variables
```{r, include=FALSE}
#for multicolinearty droped Data_drop_cor
set.seed(1234) # so that the indices will be the same when re-run
trainIndices_drop = createDataPartition(Data_drop_cor$Diagnosis, p=.8, list=F)

train_drop_raw = Data_drop_cor %>% 
  slice(trainIndices_drop)

nrow(train_drop_raw)
head(train_drop_raw)

test_drop_raw = Data_drop_cor %>% 
  slice(-trainIndices_drop)
head(test_drop_raw)

#verify if the randomization process is correct
prop.table(table(train_drop_raw$Diagnosis))
prop.table(table(test_drop_raw$Diagnosis))
```

#Raw svm with original data with default setting, data is not scaled; Using library(e1071), Default parameters: Cost =1,gamma = 1/(data dimension)
```{r svm raw, echo=TRUE}
library(e1071)
svm_raw <- svm(Diagnosis ~ ., data=train_raw, scale = FALSE, kernel =
"radial", type = "C-classification")
#svm_pca <- svm(Diagnosis ~ ., data=Data_pca)
svm_raw$gamma

pred_svm <- predict(svm_raw, test_raw)

cm_svm <- confusionMatrix(pred_svm, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm
#summary(svm_raw)
```
#Raw svm after droped correlated variables with default setting; Using library(e1071)
```{r svm_drop_raw, echo=TRUE}
svm_drop_raw <- svm(Diagnosis ~ ., data=train_drop_raw, scale = FALSE, kernel =
"radial", type = "C-classification")
#svm_pca <- svm(Diagnosis ~ ., data=Data_pca)
svm_drop_raw$gamma

pred_drop_svm <- predict(svm_drop_raw, test_drop_raw)

cm_drop_svm <- confusionMatrix(pred_drop_svm, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_drop_svm
```

#Raw caret Fitting Models with default Parameter Tuning; Using kernlab package; C = 2 ^((1:len) - 3)); C=0.25, Sigma is defined by Kernlab package's sigest function, sigest estimates based upon the 0.1 and 0.9 quantile of $||x-x'||^2$Basically any value in between those two bounds will produce good results. A vector of length 3 defining the range (0.1 quantile, median and 0.9 quantile) of the sigma hyperparameter
```{r svm caret raw, echo=TRUE}
fitControl <- trainControl(method = "none", classProbs = TRUE)
set.seed(825)
svm_raw_caret <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    trace=FALSE,
                    trControl=fitControl)
svm_raw_caret$finalModel
pred_svm_caret <- predict(svm_raw_caret, test_raw)
cm_svm_caret <- confusionMatrix(pred_svm_caret, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_caret

```
#Raw caret fitting after droped correlated variables;Fitting Models with defult Parameter Tuning; It's the same result with previous model without dropped correlated variables but it's less sensitive on positive predicting and more sensitive on negative predicting.
```{r svm caret drop raw, echo=TRUE}
fitControl <- trainControl(method = "none", classProbs = TRUE)
set.seed(825)
svm_drop_caret <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="svmRadial",
                    metric="ROC",
                    trace=FALSE,
                    trControl=fitControl)
svm_drop_caret$finalModel
pred_drop_svm_caret <- predict(svm_drop_caret, test_drop_raw)
cm_drop_svm_caret <- confusionMatrix(pred_drop_svm_caret, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_drop_svm_caret
```
#Scale variables ;
```{r svm scale, echo=TRUE}


fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_scale <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    trControl=fitControl)
pred_svm_scale <- predict(svm_scale, test_raw)
cm_svm_scale <- confusionMatrix(pred_svm_scale, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_scale

```

#Scale variables with pca 0.8;
```{r svm scale pca, echo=TRUE}


fitControl <- trainControl(method="repeatedcv",
                            number = 5,
                           repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_scale_pca <- train(Diagnosis~.,
                    data = train_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale','pca'),
                    trace=FALSE,
                    trControl=fitControl)
svm_scale_pca
pred_svm_scale_pca <- predict(svm_scale_pca, test_raw)
cm_svm_scale_pca <- confusionMatrix(pred_svm_scale_pca, as.factor(test_raw$Diagnosis), positive = "M")
cm_svm_scale_pca


```

#Scale variables with dropped variables and with pca; pca doesn't matter any more. sigest estimates based upon the 0.1 and 0.9 quantile of $||x-x'||^2$Basically any value in between those two bounds will produce good results.
```{r svm pca, echo=TRUE}
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_drop_pca  <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="svmRadial",
                    metric="ROC",
                    preProcess=c('center', 'scale','pca'),
                    trace=FALSE,
                    trControl=fitControl
                    #tuneLength = 8
                    )
svm_drop_pca
pred_svmdrop_pca <- predict(svm_drop_pca, train_drop_raw)
cm_svmdrop_pca <- confusionMatrix(pred_svmdrop_pca, as.factor(train_drop_raw$Diagnosis), positive = "M")
cm_svmdrop_pca

```

#Tune SVM parameters； The best parameters on training data doesn't guarentee the best on the test data. It would be overfitting. The final values used for the model were sigma = 0.015 and C = 1.
```{r, echo=FALSE }
svmGrid <-  expand.grid(sigma = c(.01, .015, 0.2,0.25, 0.275,0.3),
                    C = c(0.25, 0.5, 0.75, 0.9, 1, 1.1, 1.25))
                    
nrow(svmGrid)
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
svm_tune <- train(Diagnosis ~ ., 
                 data = train_drop_raw, 
                 method="svmRadial",
                 metric="ROC",
                 preProcess=c('center', 'scale','pca'),
                 trace=FALSE,
                 trControl=fitControl，
                 tuneGrid = svmGrid)
#svm_tune
#ten fold The final values used for the model were sigma = 0.015 and C = 0.9.
#five fold The final values used for the model were sigma = 0.015 and C = 0.75.
#The final values used for the model were sigma = 0.015 and C = 1.
pred_svm_tune <- predict(svm_tune, train_drop_raw)
cm_svm_tune <- confusionMatrix(pred_svm_tune, as.factor(train_drop_raw$Diagnosis), positive = "M")
cm_svm_tune

#Plotting the Resampling Profile: Examine the relationship between the estimates of performance and the tuning parameters

#trellis.par.set(caretTheme())
plot(svm_tune)  
```

#Neural network
```{r neural net scale, echo=TRUE}
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
set.seed(825)
model_nnet <- train(Diagnosis~.,
                    data = train_drop_raw,
                    method="nnet",
                    metric="ROC",
                    preProcess=c('center', 'scale'),
                    trace=FALSE,
                    tuneLength=10,
                    trControl=fitControl)
#model_nnet
pred_nnet <- predict(model_nnet, test_drop_raw)
cm_nnet <- confusionMatrix(pred_nnet, as.factor(test_drop_raw$Diagnosis), positive = "M")
cm_nnet

# #grid search for nnet
# nnetGrid <-  expand.grid(size = seq(from = 1, to = 10, by = 1),
#                         decay = seq(from = 0.1, to = 0.5, by = 0.1))
```
#Neural network with pca
```{r neural net scale pca, echo=TRUE}
set.seed(825)
fitControl <- trainControl(method="repeatedcv",
                           number = 5,
                            repeats =5,
                            preProcOptions = list(thresh = 0.8), # threshold for pca preprocess
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)
nnet_pca <- train(Diagnosis~.,
                 data = train_raw,
                 method = "nnet",
                 metric = "ROC",
                 preProcess=c('center', 'scale','pca'),
                 trace=FALSE,
                 tuneLength=10,
                 trControl = fitControl)
#nnet_pca
pred_nnet_pca <- predict(nnet_pca, test_raw)
cm_nnet_pca <- confusionMatrix(pred_nnet_pca, as.factor(test_raw$Diagnosis), positive = "M")
cm_nnet_pca 

```

#Models evaluation by caret with cross validation; training model; overfitting
```{r}
model_list <- list(
                    
                   Caret_svm_scale= svm_scale, Caret_svm_scale_pca=svm_scale_pca, 
                   Caret_drop_svm_pca=svm_drop_pca, Caret_nnet_pca=nnet_pca)
resamples <- resamples(model_list)
bwplot(resamples, metric = "ROC")
```

#Test model evaluation overall
```{r, echo=TRUE}
overall <- data.frame(model = rep(c("Svm_raw", "Svm_drop","Caret_svm_scale", "Caret_svm_scale_pca", "Caret_drop_svm_pca", "Caret_nnet_pca")),
                      rbind(cm_svm$overall,
                            cm_drop_svm$overall,
                            cm_svm_scale$overall,
                            cm_svm_scale_pca$overall,
                            cm_svmdrop_pca$overall,
                            cm_nnet_pca$overall))
#overall
overall_gather <- overall[,1:3] %>%
  gather(measure, value, Accuracy:Kappa)
overall_gather
byClass <- data.frame(model = rep(c("Svm_raw", "Svm_drop","Caret_svm_scale", "Caret_svm_scale_pca", "Caret_drop_svm_pca", "Caret_nnet_pca")),
                      rbind(cm_svm$byClass,
                            cm_drop_svm$byClass,
                            cm_svm_scale$byClass,
                            cm_svm_scale_pca$byClass,
                            cm_svmdrop_pca$byClass,
                            cm_nnet_pca$byClass))
byClass  
byClass_gather <- byClass[,1:3] %>%
  gather(measure, value, Sensitivity:Specificity)
#byClass_gather

overall_byClass_gather <- rbind(overall_gather, byClass_gather)
overall_byClass_gather <- within(overall_byClass_gather, model <- factor(model, levels = c("Svm_raw", "Svm_drop","Caret_scale", "Caret_nnet_pca", "Caret_drop_svm_pca", "Caret_svm_scale_pca")))

#overall_byClass_gather
OV<-overall_byClass_gather %>% 
  filter(measure == "Accuracy")  %>% 
  arrange(value)

#OV

Per<-ggplot(OV, aes(x = model, y = value)) +
    geom_point(shape=1)  +
    scale_y_continuous(breaks=c(0.6,0.8, 0.94, 0.96,0.97,0.98, 0.99)) 

library(plotly)
library(plyr)
library(dplyr)
Performance <- ggplotly(Per) %>%
  layout(title = 'Model performance',
         xaxis = list(title = 'model'),
         yaxis = list (title = 'Accuracy'))
Performance
```

#Conclusions:
1. Variable scale and variable dimention deduction (using PCA here) is very important. 
2. With the appropriate variable selection, it reduces the variation of dimention deduction with a stable result. 
3. Training performance could be overfitting.


